{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-Platform Project: Classifying Tweets\n",
    "\n",
    "In this off-platform project, you will use a Naive Bayes Classifier to find patterns in real tweets. Three files contain tweets that are gathered from NYC, London and Paris: `new_york.json`, `london.json`, and `paris.json`.\n",
    "\n",
    "The goal is to create a classification algorithm that can classify any tweet (or sentence) and predict whether that sentence came from New York, London, or Paris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Investigate the Data\n",
    "\n",
    "To begin, let's take a look at the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4723\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source',\n",
      "       'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'is_quote_status', 'quote_count', 'reply_count',\n",
      "       'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted',\n",
      "       'filter_level', 'lang', 'timestamp_ms', 'extended_tweet',\n",
      "       'possibly_sensitive', 'quoted_status_id', 'quoted_status_id_str',\n",
      "       'quoted_status', 'quoted_status_permalink', 'extended_entities',\n",
      "       'withheld_in_countries'],\n",
      "      dtype='object')\n",
      "Be best #ThursdayThoughts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import NYC tweet data\n",
    "new_york_tweets = pd.read_json(\"new_york.json\", lines=True)\n",
    "\n",
    "# print the number of tweets\n",
    "print(len(new_york_tweets))\n",
    "\n",
    "# print the features of a tweet\n",
    "print(new_york_tweets.columns)\n",
    "\n",
    "# print the text of the 12th tweet in the NYC dataset\n",
    "print(new_york_tweets.loc[12][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for London and Paris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5341\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source',\n",
      "       'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'is_quote_status', 'extended_tweet', 'quote_count',\n",
      "       'reply_count', 'retweet_count', 'favorite_count', 'entities',\n",
      "       'favorited', 'retweeted', 'filter_level', 'lang', 'timestamp_ms',\n",
      "       'possibly_sensitive', 'quoted_status_id', 'quoted_status_id_str',\n",
      "       'quoted_status', 'quoted_status_permalink', 'extended_entities'],\n",
      "      dtype='object')\n",
      "@JoshUJWorld @chris1989jean @richardupshall Local old people’s home please\n"
     ]
    }
   ],
   "source": [
    "# import London data\n",
    "london_tweets = pd.read_json(\"london.json\", lines=True)\n",
    "\n",
    "# print the number of tweets\n",
    "print(len(london_tweets))\n",
    "\n",
    "# print the features of a tweet\n",
    "print(london_tweets.columns)\n",
    "\n",
    "# print the text of the 20th tweet in the London dataset\n",
    "print(london_tweets.loc[20][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2510\n",
      "Index(['created_at', 'id', 'id_str', 'text', 'source', 'truncated',\n",
      "       'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
      "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
      "       'contributors', 'is_quote_status', 'quote_count', 'reply_count',\n",
      "       'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted',\n",
      "       'filter_level', 'lang', 'timestamp_ms', 'display_text_range',\n",
      "       'extended_entities', 'possibly_sensitive', 'quoted_status_id',\n",
      "       'quoted_status_id_str', 'quoted_status', 'quoted_status_permalink',\n",
      "       'extended_tweet'],\n",
      "      dtype='object')\n",
      "Une chute d’arbre sur la ligne D, une chute d’arbre sur la ligne A\n",
      "Soit la nature m’en veut, soit je sème des arbre… https://t.co/aJFYqwCcmv\n"
     ]
    }
   ],
   "source": [
    "# import Paris data\n",
    "paris_tweets = pd.read_json(\"paris.json\", lines=True)\n",
    "\n",
    "# print the number of tweets\n",
    "print(len(paris_tweets))\n",
    "\n",
    "# print the features of a tweet\n",
    "print(paris_tweets.columns)\n",
    "\n",
    "# print the text of the 134th tweet in the Paris dataset\n",
    "print(paris_tweets.loc[134]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Classifying using language: Naive Bayes Classifier\n",
    "\n",
    "We're going to create a Naive Bayes Classifier! Let's begin by looking at the way language is used differently in these three locations. Let's grab the text of all of the tweets and make it one big list.\n",
    "\n",
    "Then combine all three into a variable named `all_tweets`.\n",
    "\n",
    "Let's also make the labels associated with those tweets. `0` represents a New York tweet, `1`  represents a London tweet, and `2` represents a Paris tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab a text of the tweets in datasets\n",
    "new_york_text = new_york_tweets[\"text\"].tolist()\n",
    "london_text = london_tweets[\"text\"].tolist()\n",
    "paris_text = paris_tweets[\"text\"].tolist()\n",
    "\n",
    "# combine three lists into one\n",
    "all_tweets = new_york_text + london_text + paris_text\n",
    "\n",
    "# combine labels as well\n",
    "labels = [0] * len(new_york_text) + [1] * len(london_text) + [2] * len(paris_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Making a Training and Test Set\n",
    "\n",
    "We can now break our data into a training set and a test set. We'll use scikit-learn's `train_test_split` function to do this split. This function takes two required parameters: It takes the data, followed by the labels. Set the optional parameter `test_size` to be `0.2`. Finally, set the optional parameter `random_state` to `1`. This will make it so your data is split in the same way as the data in our solution code. \n",
    "\n",
    "`train_test_split` function returns 4 items in this order:\n",
    "1. The training data\n",
    "2. The testing data\n",
    "3. The training labels\n",
    "4. The testing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10059\n",
      "2515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(all_tweets, labels, test_size=0.2, random_state=1)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Making the Count Vectors\n",
    "\n",
    "To use a Naive Bayes Classifier, we need to transform our lists of words into count vectors. Recall that this changes the sentence `\"I love New York, New York\"` into a list that contains:\n",
    "\n",
    "* Two `1`s because the words `\"I\"` and `\"love\"` each appear once.\n",
    "* Two `2`s because the words `\"New\"` and `\"York\"` each appear twice.\n",
    "* Many `0`s because every other word in the training set didn't appear at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saying bye is hard. Especially when youre saying bye to comfort.\n",
      "  (0, 5022)\t2\n",
      "  (0, 6371)\t1\n",
      "  (0, 9552)\t1\n",
      "  (0, 12314)\t1\n",
      "  (0, 13903)\t1\n",
      "  (0, 23994)\t2\n",
      "  (0, 27146)\t1\n",
      "  (0, 29397)\t1\n",
      "  (0, 30274)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a CountVectorizer\n",
    "counter = CountVectorizer()\n",
    "\n",
    "# teach the counter our vocabulary\n",
    "counter.fit(train_data)\n",
    "\n",
    "# transform train_data and test_data into Count Vectors\n",
    "train_counts = counter.transform(train_data)\n",
    "test_counts = counter.transform(test_data)\n",
    "\n",
    "# check what a tweet looks like as a Count Vector\n",
    "print(train_data[3])\n",
    "print(train_counts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Train and Test the Naive Bayes Classifier\n",
    "\n",
    "We now have the inputs to our classifier. Let's use the CountVectors to train and test the Naive Bayes Classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# make a MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# train the classifier, fit() calculates all of the probabilities used in Bayes Theorem\n",
    "# the model is now ready to quickly predict the location of a new tweet\n",
    "classifier.fit(train_counts, train_labels)\n",
    "\n",
    "# test the model\n",
    "predictions = classifier.predict(test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Evaluating Your Model\n",
    "\n",
    "Now that the classifier has made its predictions, let's see how well it did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6779324055666004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_score = accuracy_score(test_labels, predictions)\n",
    "\n",
    "# prints the percentage of tweets in the test set that the classifier correctly classified\n",
    "print(model_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way you can evaluate your model is by looking at the **confusion matrix**. A confusion matrix is a table that describes how your classifier made its predictions. For example, if there were two labels, A and B, a confusion matrix might look like this:\n",
    "\n",
    "```\n",
    "9 1\n",
    "3 5\n",
    "```\n",
    "\n",
    "In this example, the first row shows how the classifier classified the true A's. It guessed that 9 of them were A's and 1 of them was a B. The second row shows how the classifier did on the true B's. It guessed that 3 of them were A's and 5 of them were B's.\n",
    "\n",
    "For our project using tweets, there were three classes &mdash; `0` for New York, `1` for London, and `2` for Paris. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[541 404  28]\n",
      " [203 824  34]\n",
      " [ 38 103 340]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Test Your Own Tweet\n",
    "\n",
    "Nice work! The confusion matrix should line up with your intuition. The classifier predicts tweets that were actually from New York as either New York tweets or London tweets, but almost never Paris tweets. Similarly, the classifier rarely misclassifies the tweets that were actually from Paris. Tweets coming from two English speaking countries are harder to distinguish than tweets in different languages.\n",
    "\n",
    "Now it's your chance to write a tweet and see how the classifier works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "tweet = \"Time is such a precious asset, there are so many interesting things to do in the world, no time to be lazy.\"\n",
    "\n",
    "tweet_counts = counter.transform([tweet])\n",
    "tweet_prediction = classifier.predict(tweet_counts)\n",
    "print(tweet_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
